<<<<<<< HEAD
# EthIQ â€“ Ethical Intelligence for Content Moderation  
*â€œItâ€™s not just moderation. Itâ€™s the future of ethical intelligence.â€*
=======
# EthIQ - Ethical Intelligence Platform
>>>>>>> 00cfaa9 (Rename project from AutoEthos to EthIQ - Complete rebranding with all functionality preserved)

## Overview

<<<<<<< HEAD
**EthIQ** is an AI-powered ethical moderation system designed to handle complex and sensitive content decisions with depth, transparency, and nuance. Rather than issuing binary judgments, EthIQ simulates ethical deliberation using multiple reasoning agents that represent diverse moral perspectives.
=======
EthIQ is an innovative ethical intelligence platform designed for content moderation at scale. Rather than issuing binary judgments, EthIQ simulates ethical deliberation using multiple reasoning agents and provides transparent justifications.
>>>>>>> 00cfaa9 (Rename project from AutoEthos to EthIQ - Complete rebranding with all functionality preserved)

### How It Works

A modular multi-agent system enables dynamic ethical reasoning:

- **Ethics Commander** â€“ Accepts moderation tasks and coordinates the debate process.
- **Debate Agents**:
  - `UtilitarianAgent`: Weighs harm vs. benefit  
  - `DeontologicalAgent`: Evaluates rule-based violations  
  - `CulturalContextAgent`: Considers cultural sensitivities  
  - `FreeSpeechAgent`: Assesses implications on expression and censorship  
  - `PsychologicalAgent`: Evaluates emotional and mental health impact, especially around trauma, distress, or vulnerable groups  
  - `ReligiousEthicsAgent`: Considers moral implications from diverse religious worldviews (e.g., Christianity, Islam, Buddhism)  
  - `FinancialImpactAgent`: Evaluates financial consequences at two levels:
    - **Platform-level**: Advertising risk, brand trust, compliance fines, user churn  
    - **Personal-level**: Creator monetization, user income disruption, community economic well-being  
- **Consensus Agent** â€“ Synthesizes diverse views into a final decision with clear justification

<<<<<<< HEAD
---
=======
## ðŸš€ Solution: Multi-Agent Ethical Deliberation

EthIQ enables AI to reason like ethicists, not just rule enforcers:

### ðŸ¤– Agent Architecture

1. **Ethics Commander** - Master agent that orchestrates deliberation
2. **Debate Agents** - Specialized ethical perspectives:
   - **UtilitarianAgent** - Weighs harm vs. benefit
   - **DeontologicalAgent** - Considers rule violations  
   - **CulturalContextAgent** - Evaluates cultural sensitivity
   - **FreeSpeechAgent** - Assesses freedom of expression
3. **Consensus Agent** - Synthesizes perspectives into final decision
4. **Audit Logger** - Logs process to Notion, streams metrics to Cloudera

### ðŸ”„ Deliberation Workflow

```
Content Input â†’ Individual Analysis â†’ Cross-Examination â†’ Consensus Building â†’ Final Decision
```

1. **Individual Analysis**: Each agent analyzes content from their ethical framework
2. **Cross-Examination**: Agents identify conflicts and agreements
3. **Consensus Building**: Weighted deliberation resolves disagreements
4. **Final Decision**: Transparent decision with comprehensive reasoning

## ðŸ—ï¸ Project Structure

```
nova-hackathon/
â”œâ”€â”€ agents/                 # AI agent implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_agent.py      # Base agent class
â”‚   â”œâ”€â”€ ethics_commander.py # Master orchestrator
â”‚   â”œâ”€â”€ debate_agents.py   # Ethical specialists
â”‚   â”œâ”€â”€ consensus_agent.py # Decision synthesizer
â”‚   â””â”€â”€ audit_logger.py    # Notion/Cloudera integration
â”œâ”€â”€ api/                   # FastAPI REST API
â”‚   â”œâ”€â”€ main.py           # API endpoints
â”‚   â””â”€â”€ schemas.py        # Pydantic models
â”œâ”€â”€ templates/            # Dashboard HTML templates
â”œâ”€â”€ static/              # Dashboard assets
â”‚   â”œâ”€â”€ css/
â”‚   â””â”€â”€ js/
â”œâ”€â”€ dashboard.py         # Flask web dashboard
â”œâ”€â”€ demo.py             # Demo scenarios
â”œâ”€â”€ requirements.txt    # Dependencies
â””â”€â”€ README.md          # This file
```

## ðŸ› ï¸ Installation & Setup

### Prerequisites

- Python 3.8+
- Virtual environment (recommended)

### Quick Start

1. **Clone and setup**:
   ```bash
   git clone <repository-url>
   cd nova-hackathon
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the demo**:
   ```bash
   python demo.py
   ```

4. **Start the API server**:
   ```bash
   cd api
   uvicorn main:app --host 0.0.0.0 --port 8000 --reload
   ```

5. **Start the dashboard**:
   ```bash
   python dashboard.py
   ```

### Environment Variables (Optional)

Create a `.env` file for external integrations:

```env
# Notion Integration
NOTION_TOKEN=your_notion_token
NOTION_DATABASE_ID=your_database_id

# Cloudera Integration  
CLOUDERA_HOST=your_cloudera_host
CLOUDERA_PORT=9092
CLOUDERA_TOPIC=ethiq-metrics
CLOUDERA_USERNAME=your_username
CLOUDERA_PASSWORD=your_password
```

## LLM Provider Configuration

You can configure which LLM provider to use for agent moderation:

- `LLM_PROVIDER=openai` (default, uses OpenAI GPT-3.5/4)
- `LLM_PROVIDER=gemini` (uses Google Gemini)
- `LLM_PROVIDER=mock` (returns a mock response for local/dev)

Set the following environment variables in your `.env`:

```
LLM_PROVIDER=openai  # or gemini or mock
OPENAI_API_KEY=your_openai_api_key  # for OpenAI
GOOGLE_API_KEY=your_google_api_key  # for Gemini
```

The system will automatically use the correct API and model based on your configuration.

## ðŸŽ® Usage Examples

### Command Line Demo

```bash
python demo.py
```

This runs 5 example scenarios demonstrating different ethical challenges.

### API Usage

```python
import requests

# Submit content for moderation
response = requests.post("http://localhost:8000/api/moderate", json={
    "content": "A satirical video about politics",
    "content_type": "video",
    "audience_size": 50000,
    "educational_value": True,
    "public_interest": True
})

result = response.json()
print(f"Decision: {result['final_decision']}")
print(f"Confidence: {result['confidence']}")
print(f"Reasoning: {result['reasoning']}")
```

### Web Dashboard

Visit `http://localhost:8080` for the interactive dashboard featuring:
- Real-time content moderation
- Agent status monitoring
- Analytics and trends
- Moderation history

## ðŸ”§ API Endpoints

### Content Moderation
- `POST /api/moderate` - Submit content for ethical analysis
- `GET /api/analysis/{task_id}` - Retrieve analysis results
- `GET /api/history` - View moderation history

### Agent Management
- `GET /api/agents` - List available AI agents
- `POST /api/agents/configure` - Configure agent parameters
- `GET /api/agents/status` - Check agent status

### Analytics
- `GET /api/analytics/summary` - Get moderation statistics
- `GET /api/analytics/trends` - View trend analysis
- `POST /api/analytics/export` - Export data
>>>>>>> 00cfaa9 (Rename project from AutoEthos to EthIQ - Complete rebranding with all functionality preserved)

## The Problem

Modern digital platforms struggle with ethically ambiguous content that current moderation tools are unequipped to handle. These include:

- Emotionally triggering or mentally damaging content  
- Satirical and politically charged content  
- Cultural and **religious** conflicts  
- AI-generated misinformation or harmful deepfakes  

Most moderation systems are rule-based, opaque, and inflexibleâ€”leading to inconsistent enforcement, backlash, and harm to users' psychological, cultural, spiritual, and financial well-being.

---

## Psychological Context

Content exposure affects mental and emotional health. Research shows:

- **Ambiguous or harsh content** can increase **anxiety**, **cognitive dissonance**, and **online aggression**  
- **Perceived censorship** without explanation erodes **trust** and triggers **reactance** (psychological pushback when freedom is restricted)  
- **Lack of cultural or spiritual sensitivity** leads to exclusion, identity invalidation, and offense  
- **Unmoderated misinformation** contributes to confusion, fear, and mental fatigueâ€”especially in health, politics, and religion  

The addition of the `PsychologicalAgent`, `ReligiousEthicsAgent`, and `FinancialImpactAgent` ensures moderation decisions consider mental safety, moral diversity, and both platform and user-level economics.

---

## Applications

EthIQ can ethically moderate content across a wide range of sectors:

- **Social Media & Video Platforms**: Facebook, X (Twitter), YouTube, TikTok, Reddit, etc.  
- **E-commerce & UGC Platforms**: Filter reviews, comments, and chats with ethical precision  
- **News & Media**: Moderate comments and AI content to reduce emotional harm and bias  
- **Government & Public Sector**: Combat misinformation and maintain civic trust  
- **AI Developers & Model Providers**: Ensure content generation aligns with ethical frameworks  
- **Healthcare Organizations**: Protect mental health in patient communities and filter harmful or misleading medical content  

---

## Why EthIQ Can Succeed

- Modular, explainable agent-based architecture  
- Innovative ethical debate simulation by AI  
- Tackles moderation challenges in the AI age  
- Sensitive to **psychological**, **religious**, **financial** (platform + personal), and cultural factors  
- **Built-in transparency** for trust, auditability, and defensible decision-making  

---

## ðŸ“˜ Documentation (Coming Soon)

> This section will include:

- âœ… Setup instructions  
- âœ… Usage guide and examples  
- âœ… System architecture overview  
- âœ… Agent structure and behaviors  
- âœ… API endpoints (if any)  
- âœ… Dependencies and installation steps  

*Stay tuned for the full developer documentation.*

---

> **EthIQ** enables responsible moderation that considers not just rulesâ€”but real human impact.
